{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score is 0.31285714285714283\n",
      "In Test, Number of samples predicted as irrelvant 541\n",
      "In Test, Number of samples are truely irrelvant 100\n",
      "Accuarcy Score on Original 381 classproblem is 0.20333333333333334\n",
      "Accuarcy Score on dummy class is 0.97\n",
      "In Test, Number of samples predicted as irrelvant 541\n",
      "In Test, Number of samples are truely irrelvant 100\n",
      "Precision is 0.26\n",
      "Recall is 0.9811320754716981\n",
      "F1 score is 0.4110671936758893\n"
     ]
    }
   ],
   "source": [
    "# try on new data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "global CLASS_NO\n",
    "CLASS_NO=150\n",
    "\n",
    "\n",
    "v_20_train = pickle.load(open('../../dropout_results/train_90percent.pkl','rb'))\n",
    "v_20_test = pickle.load(open('../../dropout_results/test_90percent.pkl','rb'))\n",
    "v_20_negative = pickle.load(open('../../dropout_results/negative_90percent.pkl','rb'))\n",
    "\n",
    "# df_train = pd.read_csv('/mnt/amelia_data/381classes/train.tsv', delimiter='\\t', header=None)\n",
    "# df_train.columns = ['Questions','numeric label']\n",
    "# df_train['numeric label'] = df_train['numeric label'].astype(np.int64)\n",
    "# tlabel = df_train['numeric label'].values\n",
    "\n",
    "df_test = pd.read_csv('../../alternative_data/5fold/split0/test.tsv', delimiter='\\t', header=None)\n",
    "df_test.columns = ['Questions','numeric label']\n",
    "df_test['numeric label'] = df_test['numeric label'].astype(np.int64)\n",
    "tlabel = df_test['numeric label'].values\n",
    "\n",
    "test_mean = np.mean(v_20_test,1)\n",
    "test_variance = np.std(v_20_test,1) \n",
    "\n",
    "num_split0 = 906\n",
    "\n",
    "test_mean = test_mean[:num_split0,:]\n",
    "test_variance = test_variance[:num_split0,:]\n",
    "tlabel = tlabel[:num_split0]\n",
    "\n",
    "num_negative = 4000\n",
    "\n",
    "\n",
    "negative_mean = np.mean(v_20_negative,1)\n",
    "negative_variance = np.std(v_20_negative,1)\n",
    "\n",
    "negative_mean = negative_mean[:num_negative,:]\n",
    "negative_variance = negative_variance[:num_negative,:]\n",
    "\n",
    "\n",
    "alllabels= np.concatenate((tlabel, np.ones(len(negative_mean))*150))\n",
    "\n",
    "def varianceCut_1(me, mp, cute):\n",
    "    global CLASS_NO\n",
    "    x_label=np.zeros(len(me))\n",
    "\n",
    "    for loop in range(len(me)):\n",
    "        if me[loop]>=cute:\n",
    "            x_label[loop]=CLASS_NO\n",
    "        else:\n",
    "            x_label[loop]=np.argmax(mp[loop,:])\n",
    "        \n",
    "    return x_label\n",
    "\n",
    "def varianceCut_2(mp, mv, cutp, cutv):\n",
    "    global CLASS_NO\n",
    "    \n",
    "    x_mat = np.zeros((len(mp), CLASS_NO+1))\n",
    "    x_label=np.zeros(len(mp))\n",
    "    \n",
    "    for loop in range(len(mp)):\n",
    "        assignFlag = False    \n",
    "        \n",
    "        for cloop in range(CLASS_NO):\n",
    "            if ((mp[loop, cloop]>=cutp) and (mv[loop, cloop]<=cutv)):\n",
    "                x_mat[loop,cloop]=1\n",
    "                assignFlag = True\n",
    "            \n",
    "        if not assignFlag:\n",
    "            x_mat[loop, CLASS_NO]=1\n",
    "                \n",
    "    for loop in range(len(mp)):\n",
    "        x_label[loop]=np.argmax(x_mat[loop])\n",
    "        \n",
    "    return x_label\n",
    "\n",
    "mp = np.concatenate((test_mean, negative_mean), axis=0)\n",
    "uc = np.concatenate((test_variance, negative_variance), axis=0)\n",
    "\n",
    "# apply the learned threshods of mean probability and standard deviation to data\n",
    "test_prediction = varianceCut_2(mp, uc, 0.0306, 0.0027)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('Test Accuracy Score is ' + str(accuracy_score(test_prediction, alllabels)))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==150)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==150)))\n",
    "print('Accuarcy Score on Original 381 classproblem is ' + str(accuracy_score(test_prediction[0:len(tlabel)], alllabels[0:len(tlabel)])))\n",
    "print('Accuarcy Score on dummy class is ' + str(accuracy_score(test_prediction[len(tlabel):], alllabels[len(tlabel):])))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==150)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==150)))\n",
    "binary_pred =  np.where(test_prediction < 150, 1, -1)\n",
    "binary_true =  np.where(alllabels < 150, 1, -1)\n",
    "print ('Precision is '+ str(precision_score(binary_pred, binary_true)))\n",
    "print ('Recall is '+ str(recall_score(binary_pred, binary_true)))\n",
    "print ('F1 score is '+ str(f1_score(binary_pred, binary_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score is 0.7214285714285714\n",
      "In Test, Number of samples predicted as irrelvant 128\n",
      "In Test, Number of samples are truely irrelvant 100\n",
      "Accuarcy Score on Original 381 classproblem is 0.7183333333333334\n",
      "Accuarcy Score on dummy class is 0.74\n",
      "In Test, Number of samples predicted as irrelvant 128\n",
      "In Test, Number of samples are truely irrelvant 100\n",
      "Precision is 0.91\n",
      "Recall is 0.9545454545454546\n",
      "F1 score is 0.9317406143344709\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "test_entropy = np.zeros(len(test_mean))\n",
    "negative_entropy = np.zeros(len(negative_mean))\n",
    "\n",
    "for i in range(len(test_mean)):\n",
    "    test_entropy[i]=entropy(test_mean[i,:], base=2)\n",
    "\n",
    "for i in range(len(negative_mean)):\n",
    "    negative_entropy[i]=entropy(negative_mean[i,:], base=2)\n",
    "\n",
    "me = np.concatenate((test_entropy, negative_entropy), axis=0)  \n",
    "mp = np.concatenate((test_mean, negative_mean), axis=0)\n",
    "\n",
    "#  30 iterations, for 381 classs case\n",
    "test_prediction = varianceCut_1(me, mp, 7.099)\n",
    "    \n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('Test Accuracy Score is ' + str(accuracy_score(test_prediction, alllabels)))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==150)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==150)))\n",
    "print('Accuarcy Score on Original 381 classproblem is ' + str(accuracy_score(test_prediction[0:len(tlabel)], alllabels[0:len(tlabel)])))\n",
    "print('Accuarcy Score on dummy class is ' + str(accuracy_score(test_prediction[len(tlabel):], alllabels[len(tlabel):])))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==150)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==150)))\n",
    "binary_pred =  np.where(test_prediction < 150, 1, -1)\n",
    "binary_true =  np.where(alllabels < 150, 1, -1)\n",
    "print ('Precision is '+ str(precision_score(binary_pred, binary_true)))\n",
    "print ('Recall is '+ str(recall_score(binary_pred, binary_true)))\n",
    "print ('F1 score is '+ str(f1_score(binary_pred, binary_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score is 0.8147166734610681\n",
      "In Test, Number of samples predicted as irrelvant 3317\n",
      "In Test, Number of samples are truely irrelvant 4000\n",
      "Accuarcy Score on Original 381 classproblem is 0.8642384105960265\n",
      "Accuarcy Score on dummy class is 0.8035\n",
      "In Test, Number of samples predicted as irrelvant 3317\n",
      "In Test, Number of samples are truely irrelvant 4000\n",
      "Precision is 0.8863134657836644\n",
      "Recall is 0.5053492762743864\n",
      "F1 score is 0.6436873747494991\n"
     ]
    }
   ],
   "source": [
    "# 381 class acc evaluation using dropout thresholds\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('Test Accuracy Score is ' + str(accuracy_score(test_prediction, alllabels)))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==381)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==381)))\n",
    "print('Accuarcy Score on Original 381 classproblem is ' + str(accuracy_score(test_prediction[0:len(tlabel)], alllabels[0:len(tlabel)])))\n",
    "print('Accuarcy Score on dummy class is ' + str(accuracy_score(test_prediction[len(tlabel):], alllabels[len(tlabel):])))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==381)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==381)))\n",
    "binary_pred =  np.where(test_prediction < 381, 1, -1)\n",
    "binary_true =  np.where(alllabels < 381, 1, -1)\n",
    "print ('Precision is '+ str(precision_score(binary_pred, binary_true)))\n",
    "print ('Recall is '+ str(recall_score(binary_pred, binary_true)))\n",
    "print ('F1 score is '+ str(f1_score(binary_pred, binary_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score is 0.8748471259682022\n",
      "In Test, Number of samples predicted as irrelvant 3955\n",
      "In Test, Number of samples are truely irrelvant 4000\n",
      "Accuarcy Score on Original 5 classproblem is 0.6854304635761589\n",
      "Accuarcy Score on dummy class is 0.91775\n",
      "In Test, Number of samples predicted as irrelvant 3955\n",
      "In Test, Number of samples are truely irrelvant 4000\n",
      "Precision is 0.6865342163355408\n",
      "Recall is 0.6540483701366983\n",
      "F1 score is 0.6698976844372644\n"
     ]
    }
   ],
   "source": [
    "###### for 5 class case\n",
    "\n",
    "# try on new data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "v_20_train = pickle.load(open('/mnt/amelia_data/5classes_new/10percent_train.pkl','rb'))\n",
    "v_20_test = pickle.load(open('/mnt/amelia_data/5classes_new/10percent_test.pkl','rb'))\n",
    "v_20_negative = pickle.load(open('/mnt/amelia_data/5classes_new/10percent_negative.pkl','rb'))\n",
    "\n",
    "df_test = pd.read_csv('/mnt/amelia_data/381_5_classes/split0/test.tsv', delimiter='\\t', header=None)\n",
    "df_test.columns = ['Questions','numeric label']\n",
    "df_test = df_test[pd.notnull(df_test['Questions'])] \n",
    "df_test['numeric label'] = df_test['numeric label'].astype(np.int64)\n",
    "tlabel = df_test['numeric label'].values\n",
    "\n",
    "test_mean = np.mean(v_20_test,1)\n",
    "test_variance = np.std(v_20_test,1) \n",
    "\n",
    "num_split0 = 906\n",
    "\n",
    "test_mean = test_mean[:num_split0,:]\n",
    "test_variance = test_variance[:num_split0,:]\n",
    "tlabel = tlabel[:num_split0]\n",
    "\n",
    "num_negative = 4000\n",
    "\n",
    "negative_mean = np.mean(v_20_negative,1)\n",
    "negative_variance = np.std(v_20_negative,1)\n",
    "\n",
    "negative_mean = negative_mean[:num_negative,:]\n",
    "negative_variance = negative_variance[:num_negative,:]\n",
    "\n",
    "\n",
    "alllabels= np.concatenate((tlabel, np.ones(len(negative_mean))*5))\n",
    "\n",
    "\n",
    "def varianceCut_2(mp, mv, cutp, cutv):\n",
    "    x_mat = np.zeros((len(mp), 6))\n",
    "    x_label=np.zeros(len(mp))\n",
    "    \n",
    "    for loop in range(len(mp)):\n",
    "        assignFlag = False    \n",
    "        \n",
    "        for cloop in range(5):\n",
    "            if ((mp[loop, cloop]>=cutp) and (mv[loop, cloop]<=cutv)):\n",
    "                x_mat[loop,cloop]=1\n",
    "                assignFlag = True\n",
    "            \n",
    "        if not assignFlag:\n",
    "            x_mat[loop, 5]=1\n",
    "                \n",
    "    for loop in range(len(mp)):\n",
    "        x_label[loop]=np.argmax(x_mat[loop])\n",
    "        \n",
    "    return x_label\n",
    "\n",
    "mp = np.concatenate((test_mean, negative_mean), axis=0)\n",
    "uc = np.concatenate((test_variance, negative_variance), axis=0)\n",
    "\n",
    "#test_prediction = varianceCut_2(mp, uc, 9.69437e-06, 3.06354e-06)\n",
    "\n",
    "#  10 iterations\n",
    "test_prediction = varianceCut_2(mp, uc, 0.9997, 6.19E-06)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('Test Accuracy Score is ' + str(accuracy_score(test_prediction, alllabels)))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==5)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==5)))\n",
    "print('Accuarcy Score on Original 5 classproblem is ' + str(accuracy_score(test_prediction[0:len(tlabel)], alllabels[0:len(tlabel)])))\n",
    "print('Accuarcy Score on dummy class is ' + str(accuracy_score(test_prediction[len(tlabel):], alllabels[len(tlabel):])))\n",
    "print ('In Test, Number of samples predicted as irrelvant ' + str(np.count_nonzero(test_prediction==5)))\n",
    "print ('In Test, Number of samples are truely irrelvant ' + str(np.count_nonzero(alllabels==5)))\n",
    "binary_pred =  np.where(test_prediction < 5, 1, -1)\n",
    "binary_true =  np.where(alllabels < 5, 1, -1)\n",
    "print ('Precision is '+ str(precision_score(binary_pred, binary_true)))\n",
    "print ('Recall is '+ str(recall_score(binary_pred, binary_true)))\n",
    "print ('F1 score is '+ str(f1_score(binary_pred, binary_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
